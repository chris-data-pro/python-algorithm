Module Title: Module 2: Data Collection and Storage
Module URL: "http://cisco_i2ds_mod2"
Module 2: Data Collection and Storage
Describe big data and its management.Scroll down  and select ‘Introduction’ to begin.
2.0 Introduction
2.0 Introduction
2.0 Introduction
Scroll to begin   
2.0.1 What Will I Learn in This Module?
In this module, you will learn about characteristics associated with big data, the big data pipeline, and big data management techniques. Module Title: Data Collection and StorageModule Objective: Describe big data and its management.
quickNav

2.1 Understanding Big Data
2.1 Understanding Big Data
2.1 Understanding Big Data
Scroll to begin  
2.1.1 Defining Big Data
The data analysts at Data Crunchers introduced you to the concepts of analytics and the importance of visualizations. Now it is time to work alongside the company’s data engineers to learn about big data and the role that engineers play in building, maintaining, and ensuring that the organization’s data infrastructure is available and reliable. Big data is a term used to describe the massive volumes of digital data generated, collected, and processed. The term big data describes data that is either moving too quickly, is simply too large, or is too complex to be stored, processed, or analyzed with traditional data storage and analytics applications. Some examples of big data include data generated by postings to social media accounts, such as Facebook and Twitter, and the ratings given to products on e-commerce sites like Amazon marketplace. Size is only one of the characteristics that define big data. Other criteria include the speed of generated data and the variety of data collected and stored. 
quick_nav

2.2 Understanding Big Data Management
2.2 Understanding Big Data Management
2.2 Understanding Big Data Management
Scroll to begin  
2.2.2 Lab - Interactive Widget Lab 2

quick_nav

2.3 Data Collection and Gathering Summary
2.3 Data Collection and Gathering Summary
2.3 Data Collection and Gathering Summary
Scroll to begin   
quick_nav

2.3.2 Reflection
Fast food restaurants are using big data to personalize the customer experience. Data is gathered through a mobile app, drive-thru experience, and digital menus. This data, in return, adjusts the user experience, interactions, and rewards to improve the customer experience and increase loyalty. Digital menus change depending on the weather, time of day, or local events. The menu promotes items strategically to increase sales. On a hot day, you can expect to see an image of a cold drink or food item to entice consumers. Pay attention the next time you are in a drive through to see how analytics are influencing consumer behavior.  
blank component

New Component Title

Quiz feedback

Question
Question
The four V’s of Big Data are volume, velocity, variety, and veracity. What is meant by variety?
Variety in terms of big data refers to the type of data, which is rarely in a state that is perfectly ready for processing and analysis (unstructured data).
Question
Question
What big data term includes reducing the amount of data cleaning that is required?
Big data veracity can reduce the amount of data cleaning that is required by preventing inaccurate data from spoiling data sets.
Question
Question
What is the correct order of the stages in the data pipeline?
While the exact names of the data pipeline stages may vary&#44; the process begins with taking the data from its source (ingestion)&#44; then transforming or processing the data&#44; next sending the data to its storage destination which could be on or off premises in a data warehouse or data lake&#44; and finally&#44; the data is analyzed.
2.1.2 Big Data Characteristics
Big data's characteristics change how data is collected, transmitted, stored, and accessed. Click on each quadrant in the figure below to read about the four Vs of big data and the challenges they create for the data infrastructure engineers. 
2.3.1 What Did I Learn in This Module?
In this module, you learned about big data, the characteristics associated with big data, the benefit of data growth, and the phases of a data pipeline. 
2.1.4 The Potential Benefits of Data Growth
There are many drivers of this data growth, but the most predominant include...the proliferation of Internet of Things (IoT) devices,increased internet access, greater access to broadband,use of smartphones, andthe popularity of social media.This data pool enables applications to take advantage of trends and comparisons uncovered through analytics to take action and make recommendations and reliable predictions.Select each of the following examples of how the availability of big data can benefit our society.
New Component Title

Figure

New Component Title

Scenario 1
Now that the Data Crunchers engineers have explained the various characteristics of big data, can you identify which characteristic of big data is described in each client example? 
Correct! Veracity is the characteristic that reflects the accuracy of the data being collected. The retailer is concerned that the data may not accurately reflect the majority customer opinion.
Scenario 2

Correct! Velocity reflects the speed at which the high-speed data is collected how quickly it must be analyzed in order to be effective.  
Scenario 3

Correct! Variety means that the data is gathered from many sources in different formats and that most are unstructured.
Scenario 4

Correct! Volume represents the large amount of data that must be stored, even temporarily.
What is a Data Pipeline?
Using all of this data to achieve these potential benefits requires managing the data. Data engineers are the professionals who engage in this management. This process includes developing infrastructure and systems to ingest the data, clean and transform it, and finally store it in ways that make it easy for the rest of the people in their organization to access and query the data to answer business questions. What is a data pipeline?  The best approach is to think of a data pipeline to understand better what data engineers do with data. You can think of it almost like water flowing through pipes. To understand what data engineers do with these data, consider the figure below, which is a simplified representation of data flowing through the three phases of a data pipeline: ingestion, transformation, and storage.  Note: You will also see the acronym ETL, which stands for Extract, Transform, and Load. Extract is equivalent to Ingestion and Storage is equivalent to Load.
Figure of a Data Pipeline

Data Pipeline Phases
Select each phase in the data pipeline for more information.
